#-*- coding:utf-8 -*-

import os
import cv2
import sys
import json
import time
import torch
from models import *
from utils import *
import numpy as np
import pandas as pd
import scipy.ndimage
from random import shuffle
import torch.nn.functional as F
from multiprocessing import Pool
from torch.autograd import Variable

main_ip = '../../origin_datas'
data_ip = 'image'
csv_ip = 'csv'
csv_file = os.path.join(csv_ip,'annotations_detail.csv')
resume_file='../data/classify/results-resnet-50/save_models/save_10.pth'
csv_file = '../../origin_datas/csv/annotations_detail.cev'

def padd_zero(data,size=32):
    image = np.zeros(shape=[size,size,size])
    height,width,depth = data.shape
    image[:height,:width,:depth]=data
    return image

def creat_sample(l):
    name, x, y, z, size = l
    k = size/2
    iamge = np.load(os.path.join(data_ip, name))
    data = image[y - k:y + k, x - k:x + k, z - k:z + k].copy()
    if data.shape != (size, size, size):
        data = padd_zero(data,size)
    return data

def get_evl_sets(csv_file):
    anno_info = pd.read_csv(csv_file,dtype={'ID':str})
    pool=Pool(8)
    size=32
    ids= anno_info['ID'].values
    xs = anno_info['voxelX'].values
    ys = anno_info['voxelY'].values
    zs = anno_info['voxelZ'].values
    rs = anno_info['voxelR'].values
    jobs = []
    for i, name in enumerate(ids):
        name = name + '.npy'
        jobs.append(name,(int(x[i]),int(y[i]),int(z[i]), int(rs[i]),size))
    results = pool.map(creat_sample, jobs)
    pool.close()
    pool.join()

    samples = np.array(samples)
    samples = np.expand_dims(samples,axis=1)
    samples = torch.from_numpy(samples)

    data_tensor = torch.utils.data.TensorDataset(samples) 
    data_loader = torch.utils.data.DataLoader(
        data_tensor,
        batch_size=32,
        shuffle=False,
        num_workers=8,
        pin_memory=True)
    return data_loader

model = resnet.resnet50(num_classes=2,shortcut_type='B')

model = model.cuda()
model = nn.DataParallel(model, device_ids=None)

parameters = model.parameters

print('loading checkpoint')
checkpoint = torch.load()
state_dict = checkpoint['state_dict']  
model.load_state_dict(state_dict)

import pdb
pdb.set_trace()
def predict(data_loader, model):
    print('predict')
    model.eval()
    test_results = []
    f = open('results.txt','w')
    f.write('name'+'\t'+'label'+'\t'+'prob'+'\t'+'pred'+'\n')
    for i, (names, targets) in enumerate(data_loader):
        data_time.update(time.time() - end_time)
        inputs = np.zeros(shape=[len(names),1,32,32,32])
        for k, name in enumerate(names):
            inputs[k,:] = np.load(name)
        inputs = torch.from_numpy(inputs)
        inputs = Variable(inputs, volatile=True).type(torch.float32)

        outputs = model(inputs)
        #outputs = F.softmax(outputs)

        for j in range(outputs.size(0)):
            name = os.path.basename(names[j])
            label = targets[j]
            prob, pred = torch.topk(outputs[j], k=1)
            f.write(name+','+str(label.numpy())+','+str(prob.data.cpu().numpy())+','+str(pred.data.cpu().numpy())+'\n')



        print('[{}/{}]\t'.format(i + 1,len(data_loader)))
    f.close()
